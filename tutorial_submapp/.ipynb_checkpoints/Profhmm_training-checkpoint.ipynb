{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "902b27e0",
   "metadata": {},
   "source": [
    "# Profhmm Notebook Tutorial 2023:  PART 1 / Prohmm training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd88a250",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import yaml as yml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sns\n",
    "from submapp import *\n",
    "from tools.tools_som import *\n",
    "from submapp.tools.data_processing import *\n",
    "from tools.preprocessing.preprocessor import normalization\n",
    "plt.rcParams['figure.figsize'] = [16, 12] # size of figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87761419",
   "metadata": {},
   "source": [
    "# 1) Creation of SOMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d82917",
   "metadata": {},
   "source": [
    "## 1.1) Specify data paths and parameters, and normalize data\n",
    "In this notebook we will work with model data coming from GOTM model. They were converted direclty to a np.array for convenient use.\n",
    "\n",
    "#### TO DO : \n",
    "* <font color=red>Choose size of SOM, both for surface and profile data.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090cabc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surface input data\n",
    "path_surf_data = \"./data/train_surf_2000-2019.npy\"\n",
    "path_surf_yaml = \"./data/info_surf_training.yml\"\n",
    "\n",
    "# Profile input data\n",
    "path_prof_data = \"./data/train_prof_2000-2019.npy\"\n",
    "path_prof_yaml = \"./data/info_prof_training.yml\"\n",
    "\n",
    "# Size of surface SOM\n",
    "surf_m, surf_n = 15, 15  # Ex default : 15, 15 \n",
    "\n",
    "# Size of prfoile SOM\n",
    "prof_m, prof_n = 6, 30 # Ex default : 6 ,30\n",
    "\n",
    "# Initialization method\n",
    "initialization = \"Gaussian\" # \"Gaussian\" or \"PCA\"\n",
    "\n",
    "# Choose the name of your experiment (it will be used also for prediction tutorial)\n",
    "name_experiment = \"experiment1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6eebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the numpy array saved\n",
    "surf_data = np.load(path_surf_data, allow_pickle=True)\n",
    "prof_data = np.load(path_prof_data, allow_pickle=True)\n",
    "\n",
    "# Open info yaml of data just to load usefull information\n",
    "try:\n",
    "    with open(path_surf_yaml, \"r\") as file:\n",
    "        info_surf = yml.safe_load(file)\n",
    "        file.close()\n",
    "    with open(path_prof_yaml, \"r\") as file:\n",
    "        info_prof = yml.safe_load(file)\n",
    "        file.close()\n",
    "except Exception as e:\n",
    "    print(\"Error reading the config file\")\n",
    "\n",
    "depth_levels = info_prof[\"depth\"]\n",
    "prof_var = info_prof[\"variables\"]\n",
    "prof_unit = info_prof[\"unit\"]\n",
    "\n",
    "surf_var = info_surf[\"variables\"]\n",
    "surf_unit = info_surf[\"unit\"]\n",
    "nb_blocks_train = np.shape(surf_data)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28903dba",
   "metadata": {},
   "source": [
    "``surf_data`` and ``prof_data`` are np.array with all vectors, their sizes are :\n",
    "- ``prof_data`` : **(nb_variables, nb_blocks_train)** and each element is also a np.array of size **(nb_measurement_per_year, nb_levels_depth)** \n",
    "- ``surf_data`` : **(nb_variables, nb_blocks_train)** and each element is also a np.array of size **(nb_measurement_per_year, 1)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3dfa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for surface data\n",
    "print(np.shape(surf_data))\n",
    "print(np.shape(surf_data[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41352a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example for profile data\n",
    "print(np.shape(prof_data))\n",
    "print(np.shape(prof_data[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928cc08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of the data\n",
    "surf_train_norm, surf_data_mean, surf_data_stdev,  surf_data_max, surf_data_min = normalization(nb_blocks_train, surf_data, len(surf_var))\n",
    "prof_train_norm, prof_data_mean, prof_data_stdev,  prof_data_max, prof_data_min = normalization(nb_blocks_train, prof_data, len(prof_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b178b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOM creation\n",
    "SOM_surf = som.Som(surf_n,surf_m,surf_var,len(surf_var),weights=None,name='som_surface',\n",
    "                   data_mean=surf_data_mean, data_stdev=surf_data_stdev)\n",
    "\n",
    "SOM_prof = som.Som(prof_n, prof_m, prof_var,len(depth_levels),weights=None,name='som_profile',\n",
    "                   data_mean=prof_data_mean, data_stdev=prof_data_stdev)\n",
    "\n",
    "# Initilization of the SOM weights\n",
    "if initialization == \"Gaussian\":        \n",
    "    SOM_surf.random_weights_initialization(distribution = \"gaussian\")\n",
    "    SOM_prof.random_weights_initialization(distribution = \"gaussian\")\n",
    "    \n",
    "elif initialization == \"PCA\":\n",
    "    SOM_surf.pca_weights_initialization(np.concatenate(surf_train_norm))\n",
    "    SOM_prof.pca_weights_initialization(np.concatenate(prof_train_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d937f3",
   "metadata": {},
   "source": [
    "## 1.2) SOM training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbbb6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply training for both SOM\n",
    "def training(mySom, epochs_list, nb_blocks_train, input_train):\n",
    "    print(\"------- Training for '{}' --------\".format(mySom.name))\n",
    "    print(\"------------------------------------------------\")\n",
    "    lr_initial_list, lr_final_list =  [1,1], [0.2,0.2] #Learning rate, default\n",
    "    s_initial_list, s_final_list = [6,0.1], [1,0.001] #Radius of the neighbourhood function, default\n",
    "    \n",
    "    for training_step in range(len(lr_initial_list)):\n",
    "\n",
    "        lr_initial = lr_initial_list[training_step]\n",
    "        lr_final = lr_final_list[training_step]\n",
    "\n",
    "        s_initial = s_initial_list[training_step]\n",
    "        s_final = s_final_list[training_step]\n",
    "\n",
    "        epochs = epochs_list[training_step]\n",
    "\n",
    "        param = (lr_initial,lr_final,s_initial,s_final)\n",
    "        T_train = len(np.concatenate(input_train))\n",
    "\n",
    "        for e in range(epochs):\n",
    "            print(\"#### Epoch : \", e, \" ####\")\n",
    "            for y in range(nb_blocks_train):\n",
    "                # Train the SOM with one more year from the training dataset \n",
    "                inputs = input_train[y]\n",
    "                mySom.train(\n",
    "                    data_train=inputs, \n",
    "                    param=param, \n",
    "                    T_train=T_train\n",
    "                )\n",
    "\n",
    "            # Hyperparameters can be updated after each epoch for a better control over the training\n",
    "            lr_initial=lr_final\n",
    "            lr_final=max(lr_final/1.2,0.01) # 0.01 limit value comes from common test on SOM, same for 1.2\n",
    "            s_initial=s_final\n",
    "            s_final=max(s_final/1.2,0.5) # 0.5 limit value comes from common test on SOM, same for 1.2\n",
    "            # ----------------------------\n",
    "            param = (lr_initial,lr_final,s_initial,s_final)\n",
    "            \n",
    "        print(\"------------------------------------------------\")\n",
    "        print(\"-------- Phase {} of training finished ---------\".format(training_step+1))\n",
    "        print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81069957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training both of the SOM\n",
    "training(mySom=SOM_surf, epochs_list= [4,4] , nb_blocks_train= nb_blocks_train, input_train= surf_train_norm)\n",
    "training(mySom=SOM_prof, epochs_list= [6,6] , nb_blocks_train= nb_blocks_train, input_train= prof_train_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e5d4c2",
   "metadata": {},
   "source": [
    "## 1.3) Visualize referent vectors deployment on data\n",
    "To analyze how relevant is the SOM, a good way is to check the distribution of referent vectors over the dataset. However, as humans we visualize figures in 2D, that's why we need to do projection :\n",
    "\n",
    "- As profile data has just one variable, we have chosen to use 2 main PCA axes for 2D projection.\n",
    "\n",
    "- For surface data, there are several variables, so we can do this in several 2D projection with always the variable linked to chlorphylle-a in ordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69853a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a map for surface dataset with its SOM\n",
    "map_surf = map2d.Map2d(som=SOM_surf,name=\"surf_train_mapped\")\n",
    "map_surf.map_from_data(np.concatenate(surf_train_norm))\n",
    "\n",
    "# Visualization with projection on all variables couple\n",
    "for k in range(1,len(surf_var)):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    for i in range(nb_blocks_train):\n",
    "        sns.scatterplot(x=surf_data[k][i].flatten(), y=surf_data[0][i].flatten(),color='g')\n",
    "    sns.scatterplot(x=map_surf.som.weights[:,k], y=map_surf.som.weights[:,0],color='k',label = 'Referent vectors', s = 70)\n",
    "    plt.xlabel(surf_var[k]+\" [\"+surf_unit[k]+\"]\")\n",
    "    plt.ylabel(surf_var[0]+\" [\"+surf_unit[0]+\"]\")\n",
    "    plt.title('Map deployment on training dataset {} vs {}'.format(surf_var[0],surf_var[k]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262907b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a map for profile dataset with its SOM\n",
    "map_prof = map2d.Map2d(som=SOM_prof,name=\"prof_train_mapped\")\n",
    "map_prof.map_from_data(np.concatenate(prof_train_norm))\n",
    "\n",
    "# Projection on 2 main PCA axis\n",
    "map_prof.som.print_weights_pca_axis(np.concatenate(prof_train_norm), True)\n",
    "plt.legend([\"Referent vectors\",\"Data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f345c11a",
   "metadata": {},
   "source": [
    "# 2) Creation of HMM\n",
    "The method ``init_model`` initializes the probabilities (elements of the matrices ``(Tr, Em, pi)``) by counting the transitions and emission in the training dataset.\n",
    "\n",
    "``myHMM.pi`` is initialized with a simple uniform probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ac387f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For real data we need extra-steps during HMM training which aren\"t relevant for the tutorial, \n",
    "# as we use model data.\n",
    "neighborhood, baum_welch_algo, smooth_transitions = False, False, False\n",
    "\n",
    "# HMM creation and initialization\n",
    "myHMM = hmm.Hmm(surf_n*surf_m, prof_n* prof_m, name=\"hmm_tutorial\")\n",
    "myHMM.init_model(map_surf.classes, map_prof.classes, ln_eps = -50)\n",
    "myHMM.pi = np.ones((prof_n* prof_m))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ae6139",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def transm_emi(HMM):\n",
    "    \"\"\" To see transition and emission matrix \"\"\"\n",
    "    fig= plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(HMM.Tr, annot=False)\n",
    "    plt.title(\"Transition matrix\")\n",
    "    fig= plt.figure(figsize=(10,10))\n",
    "    sns.heatmap(HMM.Em, annot=False)\n",
    "    plt.title(\"Emission matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58732296",
   "metadata": {},
   "outputs": [],
   "source": [
    "transm_emi(myHMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7448aa1",
   "metadata": {},
   "source": [
    "# 3) Save objects\n",
    "We now save our HMM object to use its properties : Transition and Emission matrix later to infer new data with the Viterbi algorithm. We also need SOM objects, to classify data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38528458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save surface HMM\n",
    "SOM_surf.clear_map_info()\n",
    "SOM_surf.map(np.concatenate(surf_train_norm))\n",
    "som.save(SOM_surf, filename = \"surf\", path=\"./objects/{}/Som/\".format(name_experiment))\n",
    "\n",
    "# Save profile SOM\n",
    "SOM_prof.clear_map_info()\n",
    "SOM_prof.map(np.concatenate(prof_train_norm))\n",
    "som.save(SOM_prof, filename = \"prof\", path=\"./objects/{}/Som/\".format(name_experiment))\n",
    "\n",
    "# Save HMM\n",
    "hmm.save(myHMM, filename=\"hmm\" ,path=\"./objects/{}/Hmm/\".format(name_experiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cbef9c",
   "metadata": {},
   "source": [
    "# 4) (Optionnal) HMM other steps, used for real data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420d976c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4.1) Neighborhood function\n",
    "In practice, there is not enough training data to\n",
    "estimate properly the elements of Em and Tr (and\n",
    "especially Tr) even with a complete set of surface\n",
    "and subsurface time-series. To overcome this problem, we use a neighbouring\n",
    "function that exploits the topological properties of the\n",
    "SOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if neighborhood:\n",
    "    outlier_threshold = 1\n",
    "    ref_distance_transitions = map_prof.som.distance_transitions\n",
    "    q = np.quantile(ref_distance_transitions, outlier_threshold)\n",
    "    ref_distance_transitions = np.where(ref_distance_transitions>q, np.nan, ref_distance_transitions)\n",
    "    dist_ref_mean = np.nanmean(ref_distance_transitions)\n",
    "\n",
    "    # Apply the neighborhood function to Tr and Em\n",
    "    myHMM.neighborhood(sigma=dist_ref_mean, distance_matrix=map_surf.som.distance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a796fe",
   "metadata": {},
   "source": [
    "## 4.2) Baum-Welch algorithm\n",
    "Given a set of observed data, the Baum–Welch algorithm uses expectation–maximization\n",
    "(EM) algorithm to find the maximum likelihood estimate of the parameters of a HMM.\n",
    "A EM algorithm is a method to find maximum likelihood estimates of parameters in a\n",
    "statistical model. The expectation (E) step gives a function for the expectation of the\n",
    "log-likelihood evaluated. The maximization (M) step, computes parameters maximiz-\n",
    "ing the expected log-likelihood found on the E step. These parameter-estimates are\n",
    "then used to determine the distribution of the latent variables in the next E step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edee3181",
   "metadata": {},
   "outputs": [],
   "source": [
    "if baum_welch_algo:\n",
    "    iterations = 30 # Default value : 10\n",
    "    print(\" #################### BAUM-WELCH ##########################\")\n",
    "    for i in range(iterations):\n",
    "        myHMM.bw(np.concatenate([map_surf.classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c42d3",
   "metadata": {},
   "source": [
    "## 4.3) Smooth transitions\n",
    "The fewer\n",
    "vertical profiles we had compared to surface data during\n",
    "the initialization the greater the impacts of B-W and\n",
    "observable time-series are. This might cause some\n",
    "discontinuity in the vertical profile reconstructed. To\n",
    "prevent this problem we used another neighbourhood\n",
    "function that decreases the probability to transit from i\n",
    "to j if they are far from each other in Mdis . Once again\n",
    "we used the same radius of in this neighbourhood\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba9a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if smooth_transitions:\n",
    "    dist_mean = np.mean(map_hid_est.distance_transitions)\n",
    "    dist_std = np.std(map_hid_est.distance_transitions)\n",
    "    if (dist_mean>dist_ref_mean):\n",
    "        print(\" #################### SMOOTH_TRANSITIONS ##########################\")\n",
    "        dist_matrix = map_hid_est.som.distance_matrix\n",
    "        myHMM.neighborhood(sigma=dist_ref_mean,\n",
    "                            distance_matrix=map_prof.som.distance_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff9ec03",
   "metadata": {},
   "source": [
    "## 4.4) Second save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84d71a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if neighborhood or baum_welch_algo or smooth_transitions:\n",
    "    name_experiment = \"experiment1\"\n",
    "\n",
    "    # Save surface HMM\n",
    "    SOM_surf.clear_map_info()\n",
    "    SOM_surf.map(np.concatenate(surf_train_norm))\n",
    "    som.save(SOM_surf, filename = \"surf\", path=\"./objects/{}/Som/\".format(name_experiment))\n",
    "\n",
    "    # Save profile SOM\n",
    "    SOM_prof.clear_map_info()\n",
    "    SOM_prof.map(np.concatenate(prof_train_norm))\n",
    "    som.save(SOM_prof, filename = \"prof\", path=\"./objects/{}/Som/\".format(name_experiment))\n",
    "\n",
    "    # Save HMM\n",
    "    hmm.save(myHMM, filename=\"hmm\" ,path=\"./objects/{}/Hmm/\".format(name_experiment))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
